"""
ë¶€ë™ì‚° ì •ì±… ë¹„êµ ë¶„ì„ Agent System
PDF ë¬¸ì„œë¥¼ ì½ê³  YAML í”„ë¡¬í”„íŠ¸ í˜•ì‹ì— ë”°ë¼ ì •ì±… ë¹„êµ ë³´ê³ ì„œë¥¼ ìë™ ìƒì„±
"""

import os
import json
import yaml
import asyncio
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

# PDF ì²˜ë¦¬
import pypdf2
import pdfplumber
from pdf2image import convert_from_path
import pytesseract

# ì„ë² ë”© ë° ë²¡í„° DB
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import PGVector
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LangGraph Agent
from langchain.agents import Tool, AgentExecutor
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from langgraph.prebuilt import ToolExecutor, ToolInvocation
from langgraph.graph import StateGraph, END

# LLM
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import get_openai_callback

class PolicyType(Enum):
    """ì •ì±… ë¬¸ì„œ íƒ€ì…"""
    LOAN_REGULATION = "ëŒ€ì¶œê·œì œ"
    HOUSING_MARKET = "ì£¼íƒì‹œì¥"
    TAX_POLICY = "ì„¸ì œì •ì±…"
    SUPPLY_POLICY = "ê³µê¸‰ì •ì±…"

@dataclass
class PolicyDocument:
    """ì •ì±… ë¬¸ì„œ ë©”íƒ€ë°ì´í„°"""
    file_path: str
    policy_date: str
    policy_type: PolicyType
    title: str
    content: str
    metadata: Dict[str, Any]

class SmartPDFLoader:
    """
    ë‹¤ì–‘í•œ PDF í˜•ì‹ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë¡œë”
    - í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF: PyPDF2 ì‚¬ìš©
    - ìŠ¤ìº” ì´ë¯¸ì§€ PDF: OCR ì‚¬ìš©
    - í‘œ ë°ì´í„° í¬í•¨: pdfplumber ì‚¬ìš©
    """
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
            length_function=len
        )
    
    def load_pdf(self, file_path: str) -> PolicyDocument:
        """PDF íŒŒì¼ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ë¡œë“œ"""
        content = ""
        metadata = {}
        
        # 1. pdfplumberë¡œ ì‹œë„ (í‘œ ë°ì´í„° ì¶”ì¶œ ê°€ëŠ¥)
        try:
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = page.extract_text()
                    if text:
                        content += f"\n[í˜ì´ì§€ {page_num + 1}]\n{text}\n"
                    
                    # í‘œ ë°ì´í„° ì¶”ì¶œ
                    tables = page.extract_tables()
                    for table_idx, table in enumerate(tables):
                        metadata[f"table_p{page_num + 1}_t{table_idx + 1}"] = table
                        # í‘œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        table_text = self._table_to_text(table)
                        content += f"\n[í‘œ {table_idx + 1}]\n{table_text}\n"
        except Exception as e:
            print(f"pdfplumber ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # 2. PyPDF2ë¡œ ì¬ì‹œë„
            try:
                with open(file_path, 'rb') as file:
                    pdf_reader = pypdf2.PdfReader(file)
                    for page_num in range(len(pdf_reader.pages)):
                        page = pdf_reader.pages[page_num]
                        text = page.extract_text()
                        content += f"\n[í˜ì´ì§€ {page_num + 1}]\n{text}\n"
            except Exception as e2:
                print(f"PyPDF2 ì²˜ë¦¬ ì‹¤íŒ¨: {e2}")
                
                # 3. OCRë¡œ ìµœì¢… ì‹œë„
                content = self._extract_with_ocr(file_path)
        
        # ì •ì±… ë‚ ì§œì™€ íƒ€ì… ìë™ ì¶”ì¶œ
        policy_date = self._extract_policy_date(content, file_path)
        policy_type = self._determine_policy_type(content, file_path)
        title = self._extract_title(content, file_path)
        
        return PolicyDocument(
            file_path=file_path,
            policy_date=policy_date,
            policy_type=policy_type,
            title=title,
            content=content,
            metadata=metadata
        )
    
    def _table_to_text(self, table: List[List]) -> str:
        """í‘œë¥¼ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not table:
            return ""
        
        text_lines = []
        for row in table:
            cleaned_row = [str(cell).strip() if cell else "" for cell in row]
            text_lines.append(" | ".join(cleaned_row))
        
        return "\n".join(text_lines)
    
    def _extract_with_ocr(self, file_path: str) -> str:
        """OCRì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            images = convert_from_path(file_path)
            content = ""
            
            for i, image in enumerate(images):
                text = pytesseract.image_to_string(image, lang='kor+eng')
                content += f"\n[í˜ì´ì§€ {i + 1}]\n{text}\n"
            
            return content
        except Exception as e:
            print(f"OCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ""
    
    def _extract_policy_date(self, content: str, file_path: str) -> str:
        """ì •ì±… ë‚ ì§œ ì¶”ì¶œ"""
        import re
        
        # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        file_date_match = re.search(r'(\d{6}|\d{4}\.\d{1,2}\.\d{1,2})', file_path)
        if file_date_match:
            return file_date_match.group(1)
        
        # ë‚´ìš©ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        date_patterns = [
            r'(\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼)',
            r'(\d{4}\.\s*\d{1,2}\.\s*\d{1,2})',
            r'(\d{2}\.\s*\d{1,2}\.\s*\d{1,2})'
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content[:1000])  # ë¬¸ì„œ ì•ë¶€ë¶„ì—ì„œ ê²€ìƒ‰
            if match:
                return match.group(1)
        
        return "ë‚ ì§œ ë¯¸ìƒ"
    
    def _determine_policy_type(self, content: str, file_path: str) -> PolicyType:
        """ì •ì±… ìœ í˜• ìë™ íŒë³„"""
        content_lower = content.lower()
        file_lower = file_path.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        if any(word in content_lower or word in file_lower 
               for word in ['ëŒ€ì¶œ', 'ltv', 'dsr', 'dti', 'ëŒ€ì¶œìˆ˜ìš”']):
            return PolicyType.LOAN_REGULATION
        elif any(word in content_lower or word in file_lower 
                for word in ['ì£¼íƒì‹œì¥', 'ë¶€ë™ì‚°ì‹œì¥', 'ì£¼íƒê°€ê²©']):
            return PolicyType.HOUSING_MARKET
        elif any(word in content_lower or word in file_lower 
                for word in ['ì„¸ì œ', 'ì·¨ë“ì„¸', 'ì¬ì‚°ì„¸', 'ì–‘ë„ì„¸']):
            return PolicyType.TAX_POLICY
        elif any(word in content_lower or word in file_lower 
                for word in ['ê³µê¸‰', 'ë¶„ì–‘', 'ì…ì£¼']):
            return PolicyType.SUPPLY_POLICY
        
        return PolicyType.HOUSING_MARKET  # ê¸°ë³¸ê°’
    
    def _extract_title(self, content: str, file_path: str) -> str:
        """ë¬¸ì„œ ì œëª© ì¶”ì¶œ"""
        import re
        
        # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ
        file_name = os.path.basename(file_path)
        title = re.sub(r'\d+_?', '', file_name)  # ìˆ«ì ì œê±°
        title = title.replace('.pdf', '').replace('_', ' ')
        
        # ë¬¸ì„œ ì²« ì¤„ì—ì„œ ì œëª© ì°¾ê¸°
        lines = content.split('\n')
        for line in lines[:10]:  # ì²« 10ì¤„ì—ì„œ ê²€ìƒ‰
            if len(line) > 10 and len(line) < 100:  # ì ì ˆí•œ ê¸¸ì´ì˜ ë¼ì¸
                if not line.startswith('['):  # í˜ì´ì§€ í‘œì‹œê°€ ì•„ë‹Œ ê²½ìš°
                    return line.strip()
        
        return title.strip()

class PolicyRetriever:
    """
    ì •ì±… ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
    - ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (Semantic Search)
    - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (Keyword Search)
    - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    """
    
    def __init__(self, connection_string: str):
        self.embeddings = OpenAIEmbeddings()
        self.connection_string = connection_string
        self.vector_store = None
        self.documents = []
        
    def initialize_vectorstore(self, documents: List[PolicyDocument]):
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        # Document ê°ì²´ë¡œ ë³€í™˜
        docs = []
        for doc in documents:
            # ì²­í¬ ë¶„í• 
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=1500,
                chunk_overlap=150
            )
            chunks = splitter.split_text(doc.content)
            
            for i, chunk in enumerate(chunks):
                docs.append(Document(
                    page_content=chunk,
                    metadata={
                        "source": doc.file_path,
                        "policy_date": doc.policy_date,
                        "policy_type": doc.policy_type.value,
                        "title": doc.title,
                        "chunk_id": i,
                        "total_chunks": len(chunks)
                    }
                ))
        
        self.documents = docs
        
        # PGVector ì´ˆê¸°í™”
        self.vector_store = PGVector.from_documents(
            documents=docs,
            embedding=self.embeddings,
            connection_string=self.connection_string,
            collection_name="policy_documents"
        )
    
    def semantic_search(self, query: str, k: int = 5) -> List[Document]:
        """ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰"""
        if not self.vector_store:
            return []
        
        return self.vector_store.similarity_search(query, k=k)
    
    def keyword_search(self, keywords: List[str], k: int = 5) -> List[Document]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        results = []
        for doc in self.documents:
            score = 0
            content_lower = doc.page_content.lower()
            
            for keyword in keywords:
                if keyword.lower() in content_lower:
                    score += content_lower.count(keyword.lower())
            
            if score > 0:
                results.append((doc, score))
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        results.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in results[:k]]
    
    def hybrid_search(self, query: str, keywords: List[str] = None, 
                      semantic_weight: float = 0.7, k: int = 5) -> List[Document]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì˜ë¯¸ + í‚¤ì›Œë“œ)"""
        # ì˜ë¯¸ ê²€ìƒ‰ ê²°ê³¼
        semantic_results = self.semantic_search(query, k=k*2)
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
        if not keywords:
            keywords = self._extract_keywords(query)
        keyword_results = self.keyword_search(keywords, k=k*2)
        
        # ê²°ê³¼ ë³‘í•© ë° ì ìˆ˜ ê³„ì‚°
        combined = {}
        
        for i, doc in enumerate(semantic_results):
            doc_id = f"{doc.metadata['source']}_{doc.metadata['chunk_id']}"
            combined[doc_id] = {
                'doc': doc,
                'score': semantic_weight * (1 - i / len(semantic_results))
            }
        
        for i, doc in enumerate(keyword_results):
            doc_id = f"{doc.metadata['source']}_{doc.metadata['chunk_id']}"
            if doc_id in combined:
                combined[doc_id]['score'] += (1 - semantic_weight) * (1 - i / len(keyword_results))
            else:
                combined[doc_id] = {
                    'doc': doc,
                    'score': (1 - semantic_weight) * (1 - i / len(keyword_results))
                }
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        sorted_results = sorted(combined.values(), key=lambda x: x['score'], reverse=True)
        return [item['doc'] for item in sorted_results[:k]]
    
    def _extract_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        import re
        
        # ì¤‘ìš” í‚¤ì›Œë“œ íŒ¨í„´
        important_terms = [
            'LTV', 'DTI', 'DSR', 'ê·œì œì§€ì—­', 'íˆ¬ê¸°ê³¼ì—´ì§€êµ¬', 'ì¡°ì •ëŒ€ìƒì§€ì—­',
            'ëŒ€ì¶œ', 'ì£¼ë‹´ëŒ€', 'ì „ì„¸ëŒ€ì¶œ', 'ì‹ ìš©ëŒ€ì¶œ', 'ì¤‘ë„ê¸ˆëŒ€ì¶œ',
            'ì£¼íƒ', 'ì•„íŒŒíŠ¸', 'ë¶„ì–‘', 'ì²­ì•½', 'ì „ë§¤ì œí•œ',
            'ì·¨ë“ì„¸', 'ì–‘ë„ì„¸', 'ì¬ì‚°ì„¸', 'ì¢…ë¶€ì„¸',
            'ìˆ˜ë„ê¶Œ', 'ì§€ë°©', 'ì„œìš¸', 'ê²½ê¸°',
            'ê¸ˆë¦¬', 'í•œë„', 'ë§Œê¸°', 'ìƒí™˜'
        ]
        
        keywords = []
        query_upper = query.upper()
        
        for term in important_terms:
            if term.upper() in query_upper:
                keywords.append(term)
        
        # ìˆ«ì íŒ¨í„´ ì¶”ì¶œ (ë‚ ì§œ, ë¹„ìœ¨ ë“±)
        numbers = re.findall(r'\d+(?:\.\d+)?[%ë…„ì›”ì¼ì–µì›]?', query)
        keywords.extend(numbers)
        
        return keywords

class PolicyAnalysisAgent:
    """
    ë¶€ë™ì‚° ì •ì±… ë¶„ì„ Agent
    YAML í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ë³´ê³ ì„œ ìƒì„±
    """
    
    def __init__(self, retriever: PolicyRetriever, llm: ChatOpenAI):
        self.retriever = retriever
        self.llm = llm
        self.memory = ConversationBufferMemory(memory_key="chat_history")
        self.yaml_prompt = None
        self.max_retries = 3
        
    def load_yaml_prompt(self, yaml_path: str):
        """YAML í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
        with open(yaml_path, 'r', encoding='utf-8') as f:
            self.yaml_prompt = yaml.safe_load(f)
    
    def create_analysis_graph(self):
        """LangGraph ë¶„ì„ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        
        # State ì •ì˜
        class AgentState(Dict):
            messages: List[str]
            current_section: str
            retrieved_docs: List[Document]
            analysis_results: Dict[str, Any]
            retry_count: int
        
        # ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(AgentState)
        
        # ë…¸ë“œ ì •ì˜
        workflow.add_node("retrieve", self.retrieve_information)
        workflow.add_node("analyze", self.analyze_section)
        workflow.add_node("validate", self.validate_analysis)
        workflow.add_node("format", self.format_report)
        
        # ì—£ì§€ ì •ì˜
        workflow.set_entry_point("retrieve")
        workflow.add_edge("retrieve", "analyze")
        workflow.add_conditional_edges(
            "analyze",
            self.should_continue_analysis,
            {
                "validate": "validate",
                "retrieve": "retrieve",
                "end": END
            }
        )
        workflow.add_conditional_edges(
            "validate",
            self.is_valid_analysis,
            {
                "format": "format",
                "retrieve": "retrieve"
            }
        )
        workflow.add_edge("format", END)
        
        return workflow.compile()
    
    def retrieve_information(self, state: Dict) -> Dict:
        """ì •ë³´ ê²€ìƒ‰ ë…¸ë“œ"""
        current_section = state.get('current_section', '')
        retry_count = state.get('retry_count', 0)
        
        # ì„¹ì…˜ë³„ ìµœì í™”ëœ ì¿¼ë¦¬ ìƒì„±
        query = self._generate_retrieval_query(current_section, retry_count)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
        docs = self.retriever.hybrid_search(query, k=10)
        
        state['retrieved_docs'] = docs
        state['retry_count'] = retry_count + 1
        
        return state
    
    def analyze_section(self, state: Dict) -> Dict:
        """ì„¹ì…˜ ë¶„ì„ ë…¸ë“œ"""
        current_section = state.get('current_section', '')
        retrieved_docs = state.get('retrieved_docs', [])
        
        if not retrieved_docs:
            state['analysis_results'][current_section] = "ì •ë³´ ì—†ìŒ"
            return state
        
        # ì„¹ì…˜ë³„ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        analysis_prompt = self._create_analysis_prompt(current_section, retrieved_docs)
        
        # LLM ë¶„ì„
        with get_openai_callback() as cb:
            response = self.llm.predict(analysis_prompt)
            
        if 'analysis_results' not in state:
            state['analysis_results'] = {}
        
        state['analysis_results'][current_section] = response
        
        return state
    
    def validate_analysis(self, state: Dict) -> Dict:
        """ë¶„ì„ ê²°ê³¼ ê²€ì¦ ë…¸ë“œ"""
        current_section = state.get('current_section', '')
        analysis_result = state.get('analysis_results', {}).get(current_section, '')
        
        # ê²€ì¦ ê¸°ì¤€
        validation_criteria = {
            'ê·œì œì§€ì—­': ['íˆ¬ê¸°ê³¼ì—´ì§€êµ¬', 'ì¡°ì •ëŒ€ìƒì§€ì—­', 'ì§€ì •', 'í•´ì œ'],
            'LTV': ['ë¹„ìœ¨', '%', 'ë¬´ì£¼íƒ', '1ì£¼íƒ', '2ì£¼íƒ'],
            'DSR': ['ìŠ¤íŠ¸ë ˆìŠ¤', 'ê¸ˆë¦¬', 'ì ìš©', 'ì‚°ì •'],
            'ëŒ€ì¶œí•œë„': ['ì–µì›', 'ì œí•œ', 'í•œë„', 'ì¶•ì†Œ'],
            'ì‹œí–‰ì‹œê¸°': ['ì‹œí–‰', 'ì ìš©', 'ê²½ê³¼ê·œì •', 'ë‚ ì§œ']
        }
        
        # ì„¹ì…˜ì— ë§ëŠ” í‚¤ì›Œë“œ í™•ì¸
        is_valid = True
        for section_key, keywords in validation_criteria.items():
            if section_key in current_section:
                if not any(kw in analysis_result for kw in keywords):
                    is_valid = False
                    break
        
        state['is_valid'] = is_valid
        return state
    
    def should_continue_analysis(self, state: Dict) -> str:
        """ë¶„ì„ ê³„ì† ì—¬ë¶€ ê²°ì •"""
        retry_count = state.get('retry_count', 0)
        
        if retry_count >= self.max_retries:
            return "end"
        
        if state.get('retrieved_docs'):
            return "validate"
        
        return "retrieve"
    
    def is_valid_analysis(self, state: Dict) -> str:
        """ë¶„ì„ ìœ íš¨ì„± í™•ì¸"""
        if state.get('is_valid', False):
            return "format"
        
        if state.get('retry_count', 0) < self.max_retries:
            return "retrieve"
        
        return "format"
    
    def format_report(self, state: Dict) -> Dict:
        """ìµœì¢… ë³´ê³ ì„œ í¬ë§·íŒ…"""
        analysis_results = state.get('analysis_results', {})
        
        # YAML í”„ë¡¬í”„íŠ¸ í˜•ì‹ì— ë§ì¶° ë³´ê³ ì„œ ìƒì„±
        report = self._format_according_to_yaml(analysis_results)
        
        state['final_report'] = report
        return state
    
    def _generate_retrieval_query(self, section: str, retry_count: int) -> str:
        """ì„¹ì…˜ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        base_queries = {
            'ì •ì±…ë°°ê²½': 'ì •ì±… ì¶”ì§„ë°°ê²½ ëª©ì  ê°€ê³„ëŒ€ì¶œ ì£¼íƒì‹œì¥ ë™í–¥',
            'ê·œì œì§€ì—­': 'íˆ¬ê¸°ê³¼ì—´ì§€êµ¬ ì¡°ì •ëŒ€ìƒì§€ì—­ í† ì§€ê±°ë˜í—ˆê°€êµ¬ì—­ ì§€ì • í•´ì œ',
            'LTV': 'LTV ì£¼íƒë‹´ë³´ëŒ€ì¶œ ë‹´ë³´ì¸ì •ë¹„ìœ¨ ë¬´ì£¼íƒ 1ì£¼íƒ 2ì£¼íƒ ì²˜ë¶„ì¡°ê±´ë¶€',
            'DSR': 'DSR DTI ì´ë¶€ì±„ì›ë¦¬ê¸ˆìƒí™˜ë¹„ìœ¨ ìŠ¤íŠ¸ë ˆìŠ¤ê¸ˆë¦¬ ì°¨ì£¼ë‹¨ìœ„',
            'ëŒ€ì¶œí•œë„': 'ì£¼ë‹´ëŒ€ í•œë„ 6ì–µì› 4ì–µì› 2ì–µì› ìƒí™œì•ˆì •ìê¸ˆ',
            'ì „ì„¸ëŒ€ì¶œ': 'ì „ì„¸ëŒ€ì¶œ ë³´ì¦ë¹„ìœ¨ ì†Œìœ ê¶Œì´ì „ì¡°ê±´ë¶€ ì „ì„¸ìê¸ˆ',
            'ì •ì±…ëŒ€ì¶œ': 'ë””ë”¤ëŒ ë²„íŒ€ëª© ë³´ê¸ˆìë¦¬ë¡  ì£¼íƒê¸°ê¸ˆ',
            'ì‹œí–‰ì‹œê¸°': 'ì‹œí–‰ì¼ ì ìš©ì‹œê¸° ê²½ê³¼ê·œì • ì˜ˆì™¸ì‚¬í•­'
        }
        
        # ì¬ì‹œë„ì‹œ ì¿¼ë¦¬ í™•ì¥
        if retry_count > 0:
            query = base_queries.get(section, section)
            query += f" ìƒì„¸ ë‚´ìš© ì¶”ê°€ ì •ë³´ {section} ê´€ë ¨"
        else:
            query = base_queries.get(section, section)
        
        return query
    
    def _create_analysis_prompt(self, section: str, docs: List[Document]) -> str:
        """ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # ë¬¸ì„œ ë‚´ìš© ê²°í•©
        context = "\n\n".join([doc.page_content for doc in docs[:5]])
        
        prompt = f"""
        ë‹¤ìŒì€ {section}ì— ëŒ€í•œ ì •ì±… ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:
        
        {context}
        
        ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í•­ëª©ë“¤ì„ ë¶„ì„í•˜ì—¬ ì •ë¦¬í•´ì£¼ì„¸ìš”:
        
        1. ì£¼ìš” ë³€ê²½ì‚¬í•­
        2. ì´ì „ ì •ì±…ê³¼ì˜ ì°¨ì´ì 
        3. ì ìš© ëŒ€ìƒ ë° ë²”ìœ„
        4. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë¹„ìœ¨
        5. ì‹œí–‰ì‹œê¸° ë° ê²½ê³¼ê·œì •
        
        í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬ê°€ í•„ìš”í•œ ê²½ìš° ë§ˆí¬ë‹¤ìš´ í‘œë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
        ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…ì‹œí•˜ì§€ ë§ˆì‹œê³ , ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ë§Œ ì •í™•íˆ ê¸°ìˆ í•´ì£¼ì„¸ìš”.
        """
        
        return prompt
    
    def _format_according_to_yaml(self, analysis_results: Dict) -> str:
        """YAML í˜•ì‹ì— ë§ì¶° ë³´ê³ ì„œ ìƒì„±"""
        if not self.yaml_prompt:
            return str(analysis_results)
        
        report = []
        
        # YAML êµ¬ì¡°ì— ë”°ë¼ ë³´ê³ ì„œ êµ¬ì„±
        for segment in ['POLICY_SUMMARY', 'POLICY_SEGMENT_01', 'POLICY_SEGMENT_02', 'POLICY_SEGMENT_03']:
            if segment in self.yaml_prompt:
                segment_content = self.yaml_prompt[segment].get('prompt', '')
                
                # í…œí”Œë¦¿ ì±„ìš°ê¸°
                for section, result in analysis_results.items():
                    if section in segment_content:
                        segment_content = segment_content.replace(f'{{{section}}}', result)
                
                report.append(segment_content)
        
        return "\n\n".join(report)
    
    def generate_comparison_report(self, 
                                 policy_files: List[str],
                                 target_area: str,
                                 main_type: str,
                                 total_units: int) -> str:
        """ì •ì±… ë¹„êµ ë³´ê³ ì„œ ìƒì„±"""
        
        # ê·¸ë˜í”„ ìƒì„±
        analysis_graph = self.create_analysis_graph()
        
        # ì„¹ì…˜ë³„ ë¶„ì„ ìˆ˜í–‰
        sections = [
            'ì •ì±…ë°°ê²½', 'ê·œì œì§€ì—­', 'LTV', 'DSR', 
            'ëŒ€ì¶œí•œë„', 'ì „ì„¸ëŒ€ì¶œ', 'ì •ì±…ëŒ€ì¶œ', 'ì‹œí–‰ì‹œê¸°'
        ]
        
        all_results = {}
        
        for section in sections:
            state = {
                'current_section': section,
                'retry_count': 0,
                'retrieved_docs': [],
                'analysis_results': {}
            }
            
            # ê·¸ë˜í”„ ì‹¤í–‰
            final_state = analysis_graph.invoke(state)
            all_results[section] = final_state.get('analysis_results', {}).get(section, '')
        
        # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        final_report = self._format_according_to_yaml(all_results)
        
        # ì‚¬ì—…ì§€ íŠ¹í™” ë¶„ì„ ì¶”ê°€
        project_analysis = self._analyze_for_project(
            all_results, target_area, main_type, total_units
        )
        
        final_report += f"\n\n## {target_area} ì‚¬ì—…ì§€ ì˜í–¥ ë¶„ì„\n{project_analysis}"
        
        return final_report
    
    def _analyze_for_project(self, 
                            analysis_results: Dict,
                            target_area: str,
                            main_type: str,
                            total_units: int) -> str:
        """íŠ¹ì • ì‚¬ì—…ì§€ì— ëŒ€í•œ ì˜í–¥ ë¶„ì„"""
        
        prompt = f"""
        ë‹¤ìŒ ì •ì±… ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ {target_area} ì§€ì—­ì˜ 
        {main_type} {total_units}ì„¸ëŒ€ ì‚¬ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
        
        {json.dumps(analysis_results, ensure_ascii=False, indent=2)}
        
        ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
        1. í•´ë‹¹ ì§€ì—­ ê·œì œ ìˆ˜ì¤€
        2. ëŒ€ì¶œ ê°€ëŠ¥ì„± ë³€í™”
        3. ìˆ˜ìš”ì êµ¬ë§¤ë ¥ ì˜í–¥
        4. ë¶„ì–‘ ì „ëµ ì œì–¸
        5. ë¦¬ìŠ¤í¬ ìš”ì¸
        """
        
        with get_openai_callback() as cb:
            response = self.llm.predict(prompt)
        
        return response


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["OPENAI_API_KEY"] = "your-api-key"
    
    # PostgreSQL ì—°ê²° ì„¤ì •
    connection_string = "postgresql://user:password@localhost:5432/policy_db"
    
    # PDF ë¡œë” ì´ˆê¸°í™”
    pdf_loader = SmartPDFLoader()
    
    # PDF íŒŒì¼ ë¡œë“œ
    policy_docs = []
    pdf_files = [
        "/mnt/user-data/uploads/251015_ëŒ€ì¶œìˆ˜ìš”_ê´€ë¦¬_ê°•í™”_ë°©ì•ˆ_ì£¼ìš”_FAQ.pdf",
        "/mnt/user-data/uploads/251015_ì£¼íƒì‹œì¥_ì•ˆì •í™”_ëŒ€ì±….pdf",
        "/mnt/user-data/uploads/0627.pdf"
    ]
    
    for pdf_file in pdf_files:
        if os.path.exists(pdf_file):
            doc = pdf_loader.load_pdf(pdf_file)
            policy_docs.append(doc)
            print(f"âœ… ë¡œë“œ ì™„ë£Œ: {doc.title} ({doc.policy_date})")
    
    # Retriever ì´ˆê¸°í™”
    retriever = PolicyRetriever(connection_string)
    retriever.initialize_vectorstore(policy_docs)
    
    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(temperature=0, model_name="gpt-4")
    
    # Agent ì´ˆê¸°í™”
    agent = PolicyAnalysisAgent(retriever, llm)
    
    # YAML í”„ë¡¬í”„íŠ¸ ë¡œë“œ (CONTEXT1ì˜ ë‚´ìš©ì„ YAML íŒŒì¼ë¡œ ì €ì¥í–ˆë‹¤ê³  ê°€ì •)
    agent.load_yaml_prompt("/home/claude/policy_prompt.yaml")
    
    # ë³´ê³ ì„œ ìƒì„±
    report = agent.generate_comparison_report(
        policy_files=pdf_files,
        target_area="ê°•ë‚¨êµ¬",
        main_type="84ã¡",
        total_units=500
    )
    
    # ë³´ê³ ì„œ ì €ì¥
    with open("/home/claude/policy_comparison_report.md", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("âœ… ì •ì±… ë¹„êµ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“„ ë³´ê³ ì„œ ìœ„ì¹˜: /home/claude/policy_comparison_report.md")

if __name__ == "__main__":
    main()
