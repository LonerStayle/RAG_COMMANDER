{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09dfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_45564\\2406965509.py:29: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  eval_llm = LangchainLLMWrapper(model)\n",
      "C:\\RAG_COMMANDER\\src\\tools\\rag\\vector_store.py:33: LangChainPendingDeprecationWarning: This class is pending deprecation and may be removed in a future version. You can swap to using the `PGVector` implementation in `langchain_postgres`. Please read the guidelines in the doc-string of this class to follow prior to migrating as there are some differences between the implementations. See <https://github.com/langchain-ai/langchain-postgres> for details about the new implementation.\n",
      "  _pgvector_cache[collection_name] = PGVector(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5fb174b9bb47e9a33c5603bf907535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS í‰ê°€ ê²°ê³¼\n",
      "====================================================================================================\n",
      "{'faithfulness': 1.0000, 'context_recall': 1.0000, 'factual_correctness(mode=f1)': 0.3350}\n",
      "\n",
      "ìƒì„¸ ê²°ê³¼:\n",
      "                              user_input  \\\n",
      "0  2025ë…„ 6ì›” 27ì¼ ì •ì±…ì—ì„œ ë””ë”¤ëŒ ëŒ€ì¶œ ì¼ë°˜ í•œë„ëŠ” ì–¼ë§ˆì¸ê°€?   \n",
      "1              10.15 ëŒ€ì±…ì—ì„œ ì„œìš¸ ê·œì œì§€ì—­ì€ ì–´ë””ì¸ê°€?   \n",
      "\n",
      "                                  retrieved_contexts  \\\n",
      "0  [í˜„í–‰ ê°œì„  ë°©ì•ˆ ì‹œí–‰ ì‹œê¸°\\nì£¼ë‹´ëŒ€\\nì´ì•¡í•œë„ ì—†ìŒ ìˆ˜ë„ê¶Œâ€¤ê·œì œì§€ì—­6ì–µì›* â€˜25...   \n",
      "1  [ï€€ í† ì§€ê±°ëž˜í—ˆê°€êµ¬ì—­ ì¶”ê°€ ì§€ì •\\nâ–¡ (ì£¼ìš”ë‚´ìš©) íˆ¬ê¸°ì  ê±°ëž˜ê°€ ì„±í–‰í•  ìš°ë ¤ê°€ ìžˆëŠ”...   \n",
      "\n",
      "                                            response           reference  \\\n",
      "0  2025ë…„ 6ì›” 27ì¼ ì •ì±…ì—ì„œ ë””ë”¤ëŒ ëŒ€ì¶œ ì¼ë°˜ í•œë„ëŠ” ìˆ˜ë„ê¶Œ 1.2ì–µì›, ì§€ë°© ...  2.5ì–µ ì›ì—ì„œ 2ì–µ ì›ìœ¼ë¡œ ì¶•ì†Œ   \n",
      "1  10.15 ëŒ€ì±…ì—ì„œ ì„œìš¸ì˜ ê·œì œì§€ì—­ì€ ì„œìš¸ì‹œ ì „ì—­ìœ¼ë¡œ, 25ê°œ ìžì¹˜êµ¬ ì „ì²´ê°€ í¬í•¨ë©...          ì„œìš¸ 25ê°œêµ¬ ì „ì²´   \n",
      "\n",
      "   faithfulness  context_recall  factual_correctness(mode=f1)  \n",
      "0           1.0             1.0                          0.00  \n",
      "1           1.0             1.0                          0.67  \n"
     ]
    }
   ],
   "source": [
    "# 3. RAGASë¥¼ ì‚¬ìš©í•œ RAG í‰ê°€\n",
    "\n",
    "from ragas import EvaluationDataset, SingleTurnSample, evaluate\n",
    "from ragas.metrics import Faithfulness, LLMContextRecall, FactualCorrectness\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tools.rag.retriever.policy_pdf_retriever import PolicyPDFRetriever\n",
    "import pandas as pd\n",
    "import cohere\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# RAGAS í‰ê°€ ë©”íŠ¸ë¦­ ì„¤ëª…:\n",
    "# - **Faithfulness (ì¶©ì‹¤ì„±)**: ìƒì„±ëœ ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ë§¥ì— ì–¼ë§ˆë‚˜ ê¸°ë°˜í•˜ê³  ìžˆëŠ”ê°€? (í™˜ê° ì—¬ë¶€)\n",
    "# - **LLMContextRecall (ë¬¸ë§¥ ë¦¬ì½œ)**: ê²€ìƒ‰ëœ ë¬¸ë§¥ì´ ì •ë‹µì„ ë§Œë“œëŠ” ë° í•„ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ”ê°€? (ê²€ìƒ‰ ëˆ„ë½ ì—¬ë¶€)\n",
    "# - **FactualCorrectness (ì‚¬ì‹¤ ì •í™•ì„±)**: ìƒì„±ëœ ë‹µë³€ì´ ì‹¤ì œ ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ê°€?\n",
    "\n",
    "# LLM ì„¤ì •\n",
    "# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "eval_llm = LangchainLLMWrapper(model)\n",
    "\n",
    "# Retriever ì´ˆê¸°í™”\n",
    "retriever = PolicyPDFRetriever()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "test_data = [\n",
    "    {\n",
    "        \"user_input\": \"2025ë…„ 6ì›” 27ì¼ ì •ì±…ì—ì„œ ë””ë”¤ëŒ ëŒ€ì¶œ ì¼ë°˜ í•œë„ëŠ” ì–¼ë§ˆì¸ê°€?\",\n",
    "        \"reference\": \"2.5ì–µ ì›ì—ì„œ 2ì–µ ì›ìœ¼ë¡œ ì¶•ì†Œ\",\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"10.15 ëŒ€ì±…ì—ì„œ ì„œìš¸ ê·œì œì§€ì—­ì€ ì–´ë””ì¸ê°€?\",\n",
    "        \"reference\": \"ì„œìš¸ 25ê°œêµ¬ ì „ì²´\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# RAGAS í‰ê°€ìš© ìƒ˜í”Œ ìƒì„±\n",
    "samples = []\n",
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "for item in test_data:\n",
    "    # 1. ì§ˆë¬¸ìœ¼ë¡œ PDF ê²€ìƒ‰ (rerankingì„ ìœ„í•´ ë” ë§Žì´ ê²€ìƒ‰)\n",
    "    query = item[\"user_input\"]\n",
    "    docs = retriever.hybrid_search(query, k=20)\n",
    "\n",
    "    # 2. Cohere Reranker ì ìš©\n",
    "    doc_texts = [doc.page_content for doc in docs]\n",
    "    rerank_results = co.rerank(\n",
    "        query=query,\n",
    "        documents=doc_texts,\n",
    "        top_n=6,  # ìµœì¢…ì ìœ¼ë¡œ 6ê°œë§Œ ì„ íƒ\n",
    "        model=\"rerank-multilingual-v3.0\",  # í•œêµ­ì–´ ì§€ì› ëª¨ë¸\n",
    "    )\n",
    "\n",
    "    # 3. Rerankëœ ìˆœì„œëŒ€ë¡œ ë¬¸ì„œ ìž¬ì •ë ¬\n",
    "    reranked_docs = [docs[result.index] for result in rerank_results.results]\n",
    "\n",
    "    # 4. ê²€ìƒ‰ëœ ë¬¸ë§¥ ì¶”ì¶œ (rerankëœ ë¬¸ì„œ ì‚¬ìš©)\n",
    "    retrieved_contexts = [doc.page_content for doc in reranked_docs]\n",
    "\n",
    "    # 5. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±\n",
    "    context_text = \"\\n\\n\".join(retrieved_contexts)\n",
    "    prompt = f\"ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\\n\\në¬¸ë§¥:\\n{context_text}\\n\\nì§ˆë¬¸: {query}\\në‹µë³€:\"\n",
    "    response = model.invoke(prompt).content\n",
    "\n",
    "    # 6. RAGAS ìƒ˜í”Œ ìƒì„±\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query,\n",
    "        retrieved_contexts=retrieved_contexts,\n",
    "        response=response,\n",
    "        reference=item[\"reference\"],\n",
    "    )\n",
    "    samples.append(sample)\n",
    "\n",
    "# í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = EvaluationDataset(samples=samples)\n",
    "\n",
    "# RAGAS í‰ê°€ ì‹¤í–‰\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[\n",
    "        Faithfulness(),\n",
    "        LLMContextRecall(),\n",
    "        FactualCorrectness(),\n",
    "    ],\n",
    "    llm=eval_llm,\n",
    ")\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"RAGAS í‰ê°€ ê²°ê³¼\")\n",
    "print(\"=\" * 100)\n",
    "print(result)\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ìƒì„¸ ê²°ê³¼ í™•ì¸\n",
    "df = result.to_pandas()\n",
    "print(\"\\nìƒì„¸ ê²°ê³¼:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7022d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RAGASë¥¼ ì‚¬ìš©í•œ RAG í‰ê°€\n",
    "\n",
    "from ragas import EvaluationDataset, SingleTurnSample, evaluate\n",
    "from ragas.metrics import Faithfulness, LLMContextRecall, FactualCorrectness\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tools.rag.retriever.policy_pdf_retriever import PolicyPDFRetriever\n",
    "import pandas as pd\n",
    "import cohere\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# -------------cohere\n",
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "\n",
    "# -------------\n",
    "# RAGAS í‰ê°€ ë©”íŠ¸ë¦­ ì„¤ëª…:\n",
    "# - **Faithfulness (ì¶©ì‹¤ì„±)**: ìƒì„±ëœ ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ë§¥ì— ì–¼ë§ˆë‚˜ ê¸°ë°˜í•˜ê³  ìžˆëŠ”ê°€? (í™˜ê° ì—¬ë¶€)\n",
    "# - **LLMContextRecall (ë¬¸ë§¥ ë¦¬ì½œ)**: ê²€ìƒ‰ëœ ë¬¸ë§¥ì´ ì •ë‹µì„ ë§Œë“œëŠ” ë° í•„ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ”ê°€? (ê²€ìƒ‰ ëˆ„ë½ ì—¬ë¶€)\n",
    "# - **FactualCorrectness (ì‚¬ì‹¤ ì •í™•ì„±)**: ìƒì„±ëœ ë‹µë³€ì´ ì‹¤ì œ ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ê°€?\n",
    "\n",
    "# LLM ì„¤ì •\n",
    "# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "eval_llm = LangchainLLMWrapper(model)\n",
    "\n",
    "# Retriever ì´ˆê¸°í™”\n",
    "retriever = PolicyPDFRetriever()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "test_data = [\n",
    "    {\n",
    "        \"user_input\": \"2025ë…„ 6ì›” 27ì¼ ì •ì±…ì—ì„œ ë””ë”¤ëŒ ëŒ€ì¶œ ì¼ë°˜ í•œë„ëŠ” ì–¼ë§ˆì¸ê°€?\",\n",
    "        \"reference\": \"2.5ì–µ ì›ì—ì„œ 2ì–µ ì›ìœ¼ë¡œ ì¶•ì†Œ\",\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"10.15 ëŒ€ì±…ì—ì„œ ì„œìš¸ ê·œì œì§€ì—­ì€ ì–´ë””ì¸ê°€?\",\n",
    "        \"reference\": \"ì„œìš¸ 25ê°œêµ¬ ì „ì²´\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# RAGAS í‰ê°€ìš© ìƒ˜í”Œ ìƒì„±\n",
    "samples = []\n",
    "\n",
    "for item in test_data:\n",
    "    # 1. ì§ˆë¬¸ìœ¼ë¡œ PDF ê²€ìƒ‰ (rerankingì„ ìœ„í•´ ë” ë§Žì´ ê²€ìƒ‰)\n",
    "    query = item[\"user_input\"]\n",
    "    docs = retriever.hybrid_search(query, k=20)\n",
    "\n",
    "    # 2. Cohere Reranker ì ìš©\n",
    "    doc_texts = [doc.page_content for doc in docs]\n",
    "    rerank_results = co.rerank(\n",
    "        query=query,\n",
    "        documents=doc_texts,\n",
    "        top_n=6,  # ìµœì¢…ì ìœ¼ë¡œ 6ê°œë§Œ ì„ íƒ\n",
    "        model=\"rerank-multilingual-v3.0\",  # í•œêµ­ì–´ ì§€ì› ëª¨ë¸\n",
    "    )\n",
    "\n",
    "    # 3. Rerankëœ ìˆœì„œëŒ€ë¡œ ë¬¸ì„œ ìž¬ì •ë ¬\n",
    "    reranked_docs = [docs[result.index] for result in rerank_results.results]\n",
    "\n",
    "    # 4. ê²€ìƒ‰ëœ ë¬¸ë§¥ ì¶”ì¶œ (rerankëœ ë¬¸ì„œ ì‚¬ìš©)\n",
    "    retrieved_contexts = [doc.page_content for doc in reranked_docs]\n",
    "\n",
    "    # 5. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±\n",
    "    context_text = \"\\n\\n\".join(retrieved_contexts)\n",
    "    prompt = f\"ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\\n\\në¬¸ë§¥:\\n{context_text}\\n\\nì§ˆë¬¸: {query}\\në‹µë³€:\"\n",
    "    response = model.invoke(prompt).content\n",
    "\n",
    "    # 6. RAGAS ìƒ˜í”Œ ìƒì„±\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=query,\n",
    "        retrieved_contexts=retrieved_contexts,\n",
    "        response=response,\n",
    "        reference=item[\"reference\"],\n",
    "    )\n",
    "    samples.append(sample)\n",
    "\n",
    "# í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±\n",
    "dataset = EvaluationDataset(samples=samples)\n",
    "\n",
    "# RAGAS í‰ê°€ ì‹¤í–‰\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[\n",
    "        Faithfulness(),\n",
    "        LLMContextRecall(),\n",
    "        FactualCorrectness(),\n",
    "    ],\n",
    "    llm=eval_llm,\n",
    ")\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"RAGAS í‰ê°€ ê²°ê³¼\")\n",
    "print(\"=\" * 100)\n",
    "print(result)\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ìƒì„¸ ê²°ê³¼ í™•ì¸\n",
    "df = result.to_pandas()\n",
    "print(\"\\nìƒì„¸ ê²°ê³¼:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5dd3e",
   "metadata": {},
   "source": [
    "## ì„±ëŠ¥ ë¹„êµ: Reranker ì ìš© ì „ vs í›„\n",
    "\n",
    "ì•„ëž˜ ì…€ì„ ì‹¤í–‰í•˜ë©´ Rerankerë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²½ìš°ì™€ ì‚¬ìš©í•œ ê²½ìš°ì˜ ì„±ëŠ¥ì„ ë¹„êµí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranker ì ìš© ì „í›„ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "from ragas import EvaluationDataset, SingleTurnSample, evaluate\n",
    "from ragas.metrics import Faithfulness, LLMContextRecall, FactualCorrectness\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tools.rag.retriever.policy_pdf_retriever import PolicyPDFRetriever\n",
    "import pandas as pd\n",
    "import cohere\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# ì´ˆê¸°í™”\n",
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "eval_llm = LangchainLLMWrapper(model)\n",
    "retriever = PolicyPDFRetriever()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "test_data = [\n",
    "    {\n",
    "        \"user_input\": \"2025ë…„ 6ì›” 27ì¼ ì •ì±…ì—ì„œ ë””ë”¤ëŒ ëŒ€ì¶œ ì¼ë°˜ í•œë„ëŠ” ì–¼ë§ˆì¸ê°€?\",\n",
    "        \"reference\": \"ìˆ˜ë„ê¶Œ 1.2ì–µ ì›, ì§€ë°© 0.8ì–µ ì›\",\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"10.15 ëŒ€ì±…ì—ì„œ ì„œìš¸ ê·œì œì§€ì—­ì€ ì–´ë””ì¸ê°€?\",\n",
    "        \"reference\": \"ì„œìš¸ 25ê°œêµ¬ ì „ì²´\",\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"ìƒì• ìµœì´ˆ ì£¼íƒêµ¬ìž…ìžì˜ LTVëŠ” ëª‡ í¼ì„¼íŠ¸ì¸ê°€?\",\n",
    "        \"reference\": \"70%\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def run_evaluation(use_reranker=False, test_name=\"\"):\n",
    "    \"\"\"í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    samples = []\n",
    "\n",
    "    for item in test_data:\n",
    "        query = item[\"user_input\"]\n",
    "\n",
    "        if use_reranker:\n",
    "            # Reranker ì ìš©\n",
    "            docs = retriever.hybrid_search(query, k=20)\n",
    "            doc_texts = [doc.page_content for doc in docs]\n",
    "            rerank_results = co.rerank(\n",
    "                query=query,\n",
    "                documents=doc_texts,\n",
    "                top_n=6,\n",
    "                model=\"rerank-multilingual-v3.0\",\n",
    "            )\n",
    "            reranked_docs = [docs[result.index] for result in rerank_results.results]\n",
    "            retrieved_contexts = [doc.page_content for doc in reranked_docs]\n",
    "        else:\n",
    "            # Reranker ë¯¸ì ìš©\n",
    "            docs = retriever.hybrid_search(query, k=6)\n",
    "            retrieved_contexts = [doc.page_content for doc in docs]\n",
    "\n",
    "        # LLM ë‹µë³€ ìƒì„±\n",
    "        context_text = \"\\n\\n\".join(retrieved_contexts)\n",
    "        prompt = f\"ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\\n\\në¬¸ë§¥:\\n{context_text}\\n\\nì§ˆë¬¸: {query}\\në‹µë³€:\"\n",
    "        response = model.invoke(prompt).content\n",
    "\n",
    "        # ìƒ˜í”Œ ìƒì„±\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=query,\n",
    "            retrieved_contexts=retrieved_contexts,\n",
    "            response=response,\n",
    "            reference=item[\"reference\"],\n",
    "        )\n",
    "        samples.append(sample)\n",
    "\n",
    "    # í‰ê°€ ì‹¤í–‰\n",
    "    dataset = EvaluationDataset(samples=samples)\n",
    "    result = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=[\n",
    "            Faithfulness(),\n",
    "            LLMContextRecall(),\n",
    "            FactualCorrectness(),\n",
    "        ],\n",
    "        llm=eval_llm,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ“Š {test_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(result)\n",
    "    print(f\"\\nìƒì„¸ ê²°ê³¼:\")\n",
    "    df = result.to_pandas()\n",
    "    print(\n",
    "        df[\n",
    "            [\n",
    "                \"user_input\",\n",
    "                \"faithfulness\",\n",
    "                \"context_recall\",\n",
    "                \"factual_correctness(mode=f1)\",\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ì‹¤í–‰\n",
    "print(\"ðŸ”µ Reranker ë¯¸ì ìš© í…ŒìŠ¤íŠ¸ ì‹œìž‘...\")\n",
    "result_without_reranker = run_evaluation(\n",
    "    use_reranker=False, test_name=\"Reranker ë¯¸ì ìš©\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŸ¢ Reranker ì ìš© í…ŒìŠ¤íŠ¸ ì‹œìž‘...\")\n",
    "result_with_reranker = run_evaluation(use_reranker=True, test_name=\"Reranker ì ìš©\")\n",
    "\n",
    "# ë¹„êµ ìš”ì•½\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ðŸ“ˆ ì„±ëŠ¥ ë¹„êµ ìš”ì•½\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "metrics_comparison = {\n",
    "    \"Faithfulness\": [\n",
    "        result_without_reranker[\"faithfulness\"],\n",
    "        result_with_reranker[\"faithfulness\"],\n",
    "    ],\n",
    "    \"Context Recall\": [\n",
    "        result_without_reranker[\"context_recall\"],\n",
    "        result_with_reranker[\"context_recall\"],\n",
    "    ],\n",
    "    \"Factual Correctness\": [\n",
    "        result_without_reranker[\"factual_correctness(mode=f1)\"],\n",
    "        result_with_reranker[\"factual_correctness(mode=f1)\"],\n",
    "    ],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(\n",
    "    metrics_comparison, index=[\"Reranker ë¯¸ì ìš©\", \"Reranker ì ìš©\"]\n",
    ")\n",
    "\n",
    "print(\"\\nì„±ëŠ¥ ë¹„êµí‘œ:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# ê°œì„ ìœ¨ ê³„ì‚°\n",
    "print(\"\\nê°œì„ ìœ¨:\")\n",
    "for metric in metrics_comparison.keys():\n",
    "    before = metrics_comparison[metric][0]\n",
    "    after = metrics_comparison[metric][1]\n",
    "    improvement = ((after - before) / before * 100) if before > 0 else 0\n",
    "    print(f\"{metric}: {improvement:+.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_COMMANDER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
