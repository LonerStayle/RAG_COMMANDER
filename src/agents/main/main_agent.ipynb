{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3e470f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/seobi/PythonProjects/RAG_Commander/src\n",
      "/Users/seobi/PythonProjects/RAG_Commander/src/agents/main\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "src_path = Path(os.getcwd()).resolve().parents[1]  \n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "print(sys.path[-1])  \n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efa8f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main_agent.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph.state import Command, Literal\n",
    "from agents.state.start_state import StartConfirmation, StartInput\n",
    "from agents.state.main_state import MainState\n",
    "from utils.enum import LLMProfile\n",
    "from utils.util import get_today_str\n",
    "from langchain_core.messages import HumanMessage, get_buffer_string, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from prompts import PromptManager, PromptType\n",
    "from agents.analysis.analysis_graph import analysis_graph\n",
    "from agents.jung_min_jae.jung_min_jae_agent import report_graph\n",
    "from copy import deepcopy\n",
    "\n",
    "start_llm = ChatOpenAI(model=LLMProfile.START_COMFIRMATION, temperature=0)\n",
    "messages_key = MainState.KEY.messages\n",
    "start_input_key = MainState.KEY.start_input\n",
    "analysis_outputs_key = MainState.KEY.analysis_outputs\n",
    "status_key = MainState.KEY.status\n",
    "\n",
    "\n",
    "def start_confirmation(\n",
    "    state: MainState,\n",
    ") -> Command[Literal[\"start\", \"__end__\"]]:\n",
    "\n",
    "    parser_llm = start_llm.with_structured_output(StartConfirmation)\n",
    "    messages_str = get_buffer_string(messages=state[messages_key])\n",
    "\n",
    "    prompt = PromptManager(PromptType.MAIN_START_CONFIRMATION).get_prompt(\n",
    "        messages=messages_str\n",
    "    )\n",
    "    response: StartConfirmation = parser_llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    if response.confirm == False:\n",
    "        return Command(\n",
    "            goto=END, update={messages_key: [AIMessage(content=response.question)]}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"start\",\n",
    "            update={messages_key: [AIMessage(content=response.verification)]},\n",
    "        )\n",
    "\n",
    "\n",
    "def start(state: MainState) -> MainState:\n",
    "    parser_model = start_llm.with_structured_output(StartInput)\n",
    "    prompt = PromptManager(PromptType.MAIN_START).get_prompt(\n",
    "        messages=get_buffer_string(state[messages_key]), date=get_today_str()\n",
    "    )\n",
    "    response: StartInput = parser_model.invoke([HumanMessage(content=prompt)])\n",
    "    return {start_input_key: response.model_dump(), status_key: \"ANALYSIS\"}\n",
    "\n",
    "\n",
    "def analysis_graph_node(state: MainState) -> MainState:\n",
    "    result = analysis_graph.invoke({\"start_input\": deepcopy(state[start_input_key])})\n",
    "    return {\n",
    "        \"analysis_outputs\": result.get(\"analysis_outputs\", {}),\n",
    "        status_key: \"JUNG_MIN_JAE\"\n",
    "    }\n",
    "\n",
    "\n",
    "def jung_min_jae_graph(state: MainState) -> MainState:\n",
    "    result = report_graph.invoke({\"start_input\": deepcopy(state[start_input_key]),\n",
    "                                  \"analysis_outputs\": deepcopy(state[analysis_outputs_key])})\n",
    "    return {\n",
    "        \"final_report\": result[\"final_report\"],\n",
    "        status_key:\"RENDERING\"\n",
    "    }\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(MainState)\n",
    "\n",
    "start_confirmation_key = \"start_confirmation\"\n",
    "start_key = \"start\"\n",
    "analysis_graph_key = \"analysis_graph\"\n",
    "jung_min_jae_key = \"jung_min_jae_graph\"\n",
    "\n",
    "graph_builder.add_node(start_confirmation_key, start_confirmation)\n",
    "graph_builder.add_node(start_key, start)\n",
    "graph_builder.add_node(analysis_graph_key, analysis_graph_node)\n",
    "graph_builder.add_node(jung_min_jae_key, jung_min_jae_graph)\n",
    "\n",
    "graph_builder.add_edge(START, start_confirmation_key)\n",
    "graph_builder.add_edge(start_key, analysis_graph_key)\n",
    "graph_builder.add_edge(analysis_graph_key, jung_min_jae_key)\n",
    "graph_builder.add_edge(analysis_graph_key, END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0533bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seobi/PythonProjects/RAG_Commander/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to import langchain_anthropic. Please install with `pip install -U langchain-anthropic`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmain_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph_builder\n\u001b[32m      2\u001b[39m graph = graph_builder.compile()\n\u001b[32m      3\u001b[39m graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonProjects/RAG_Commander/src/agents/main/main_agent.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptManager, PromptType\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalysis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalysis_graph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m analysis_graph\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjung_min_jae\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjung_min_jae_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m report_graph\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[32m     14\u001b[39m start_llm = ChatOpenAI(model=LLMProfile.START_COMFIRMATION, temperature=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonProjects/RAG_Commander/src/agents/jung_min_jae/jung_min_jae_agent.py:86\u001b[39m\n\u001b[32m     83\u001b[39m final_report_key = JungMinJaeState.KEY.final_report\n\u001b[32m     84\u001b[39m messages_key = JungMinJaeState.KEY.messages\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m llm = \u001b[43minit_chat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLLMProfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREPORT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m tool_list = [think_tool]\n\u001b[32m     88\u001b[39m llm_with_tools = llm.bind_tools(tool_list)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonProjects/RAG_Commander/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py:291\u001b[39m, in \u001b[36minit_chat_model\u001b[39m\u001b[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m     warnings.warn(\n\u001b[32m    284\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_prefix\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m has been set but no fields are configurable. Set \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`configurable_fields=(...)` to specify the model params that are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconfigurable.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    287\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    288\u001b[39m     )\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configurable_fields:\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_chat_model_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[32m    297\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m] = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonProjects/RAG_Commander/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py:320\u001b[39m, in \u001b[36m_init_chat_model_helper\u001b[39m\u001b[34m(model, model_provider, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatOpenAI(model=model, **kwargs)\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_provider == \u001b[33m\"\u001b[39m\u001b[33manthropic\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     \u001b[43m_check_pkg\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlangchain_anthropic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_anthropic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatAnthropic\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatAnthropic(model=model, **kwargs)  \u001b[38;5;66;03m# type: ignore[call-arg,unused-ignore]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PythonProjects/RAG_Commander/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py:500\u001b[39m, in \u001b[36m_check_pkg\u001b[39m\u001b[34m(pkg, pkg_kebab)\u001b[39m\n\u001b[32m    498\u001b[39m pkg_kebab = pkg_kebab \u001b[38;5;28;01mif\u001b[39;00m pkg_kebab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pkg.replace(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    499\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Please install with `pip install -U \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg_kebab\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n",
      "\u001b[31mImportError\u001b[39m: Unable to import langchain_anthropic. Please install with `pip install -U langchain-anthropic`"
     ]
    }
   ],
   "source": [
    "from main_agent import graph_builder\n",
    "graph = graph_builder.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.format_message import format_message\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from agents.state.main_state import MainState\n",
    "messages_key = MainState.KEY.messages\n",
    "checkpointer = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer = checkpointer)\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\":\"1\"}}\n",
    "result = graph.invoke(\n",
    "    {\n",
    "        messages_key : [HumanMessage(content = \"경기도 분당구 정자동 백현로 206 근처를 분석하고 싶고 규모는 대단지, 세대수는 1000세대 정도 생각합니다. \")]\n",
    "    },\n",
    "    config = thread\n",
    ")\n",
    "\n",
    "format_message(result[messages_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf2917",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d5bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1670b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['final_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d5f644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-commander",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
